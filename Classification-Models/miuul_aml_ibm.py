# -*- coding: utf-8 -*-
"""Miuul_AML_IBM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13siG7f-WwNw6DtTPqaqzWWzdVvjdsT8R

**KARA PARA AKLAMA (AML)**
“Resmi” ekonominin yanı sıra, parasal bir avantaj elde etmek için iş dünyasının yasal kıldığı
çerçevenin dışında hareket etmeye çalışarak kriminal ekonomiyi karakterize eden bir “yeraltı
ekonomisi” bulunmaktadır. Yasadışı faaliyetlerin yürütüldüğü bu suç ekonomisi, organize suçlar
veya uyuşturucu ticareti gibi yasal sistemle çelişen her türlü suç faaliyetini kapsamaktadır. Suç
ekonomisi içerisinde elde edilen kazançlar ise kara para aklama sürecinin amacını oluşturmaktadır.
Yasadışı faaliyetlerden sağlanan bu paraları tekrar kullanmadan önce legal işletmelerin bünyesine
alarak bir tür “aklama” yapılması, suçluların yakalanmamaları açısından gereklidir. Bu işleme, vergi
dolandırıcılığı veya sermaye kaçırma ile karıştırılmaması gereken “kara para aklama (money
laundering)” denmektedir.

Kara para aklama, suçtan elde edilen gelire meşru bir görünüm verir ve böylece illegal paranın finans
sektöründe anonim olarak yasal paralarla karışmasını sağlar. Diğer bir ifadeyle, kara para aklama
gayri meşru geliri meşru gösterme sürecidir

Aklanacak tüm gayrimeşru paralar finansal sisteme dâhil edilmeyi beklemektedir ve bu nedenle tanım
gereği bunların bankalardan bir şekilde geçmesi gereklidir. Bu yüzden, bankacılık sektörü genellikle
kara para aklamayla mücadele girişimlerinin odak noktasıdır.

Sistemlerinde bir miktar kirli para olduğunu bilirler ancak bunun temiz paradan
ayrışması kolay değildir. Buradan hareketle, kirli paranın tespit edilmesi genellikle finansal sisteme
ilk girişinde mümkün olmaktadır
[https://dergipark.org.tr/tr/download/article-file/3877329]
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
#import kagglehub
#ealtman2019_ibm_transactions_for_anti_money_laundering_aml_path = kagglehub.dataset_download('ealtman2019/ibm-transactions-for-anti-money-laundering-aml')

#print('Data source import complete.')

import pandas as pd
import numpy as np
import os
from numpy import concatenate
from math import sqrt

import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 170)
pd.set_option('display.max_rows', None)
pd.set_option('display.float_format', lambda x: '%.3f' % x)

df = pd.read_csv("/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans.csv")

#df_account = pd.read_csv("/kaggle/input/ibm-transactions-for-anti-money-laundering-aml/HI-Small_accounts.csv")

df.head()

#df_account.head()

"""# **Veri Seti Hikayesi**

IBM AML verisi sentetik bir veridir. 2022 yılının Eylül ayında gerçekleştirilen para aktarma işlemlerini incelenmektedir


*   Timestamp: İşlemin gerçekleştiği zaman

*   From Bank: Paranın gönderildiği banka ID.

*   Account: Parayı gönderen kişinin hesap numarası.

*   To Bank: Paranın gönderildiği banka ID.

*   Account.1: Parayı alan kişinin hesap numarasıdır.

*   Amount Received: Alıcı banka hesabına geçen paranın miktarıdır.

*   Receiving Currency: Alıcının hesabına geçen paranın para birimidir .

*   Amount Paid: Gönderici tarafından gönderilen paranın miktarıdır.

*   Payment Currency: Göndericinin işlemi yaptığı para birimidir.

*   Payment Format: İşlemin nasıl yapıldığını gösteren format bilgisidir (örneğin, SWIFT, EFT, havale).

*   Is Laundering: İşlemin kara para olup olmadığını belirten hedef değişkendir. 1:Kara Para  0: Kara para değil

# **1-Keşifçi Veri Analizi EDA**
"""

# df.info()
# #df_account.info()
# # 2 farklı df var. Birisi işlem yapılanlar diğeri banka hesabının olduğu df.
# # Soru: İlk df'de eksik veriler var. Hiç işlem yapmayan bankalar 2. df'de olup,
# # buraya kara para akıyor olabilir mi? Hiç işlem yapmadığı için 1. df'de görünmüyor
# # olabilir mi?

# for i in df.columns:
#   print(i)
#   print(df[i].nunique())

# def check_df(dataframe, head=5):
#     print("##################### Shape #####################")
#     print(dataframe.shape)
#     print("##################### Types #####################")
#     print(dataframe.dtypes)
#     print("##################### Head #####################")
#     print(dataframe.head(head))
#     print("##################### Tail #####################")
#     print(dataframe.tail(head))
#     print("##################### NA #####################")
#     print(dataframe.isnull().sum())
#     print("##################### Quantiles #####################")
#     #print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

#  check_df(df)

df['Is Laundering'].value_counts()#dengesiz bir veri seti

"""Bu dengesizliği aşağıdaki gibi görselleştirebiliriz"""

# target_count=df["Is Laundering"].value_counts()
# plt.subplot(1,2,1)
# sns.barplot(x=target_count.index,y=target_count.values)
# plt.subplot(1,2,2)
# plt.pie(target_count.values,labels=target_count.index);

# df["Amount Paid"].quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T #Max değer gerçekten anormal bir durum mudur?
# #Sonsuza yakın bu sayı cinsi analizlerimize göre aslında Yen olduğu için görünürde böyle.
# #Peki reel olarak çok yüksek miktarlarda para girişi ve çıkışı Launder olabilir mi?

# # Histogram çizimi
# plt.figure(figsize=(15, 8))
# plt.hist(df["Amount Paid"], bins=3000, edgecolor='black', color='salmon')
# plt.title('Amount Paid Dağılımı (X Ekseni Logaritmik Ölçek)')
# plt.xlabel('Amount Paid (Log Ölçek)')
# plt.ylabel('Frekans')
# #plt.xscale('log')
# plt.yscale('log')
# plt.grid(True)
# plt.show()

# df["Amount Received"].quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T

# plt.figure(figsize=(15, 8))
# plt.hist(df["Amount Received"], bins=3000, edgecolor='black', color='salmon')
# plt.title('Amount Received Dağılımı')
# plt.xlabel('Amount Received ')
# plt.ylabel('Frekans')
# #plt.xscale('log')
# plt.yscale('log')
# plt.grid(True)
# plt.show()

# df[df["Amount Paid"] == df["Amount Paid"].max()] #Yüksek para giriş ve çıkışları bankacılıkta normal durum olabilir
# #Örneğin:
# #bank to bank
# #gov to gov
# #Gibi işlemler normal karşılanır.

# df["Payment Format"].value_counts() #Ödeme tipleri dağılımı

# df.groupby(["Payment Currency"]).aggregate({"Amount Paid":"max"}) #Currency dağılımı

# df.groupby("Payment Currency")["Amount Paid"].quantile([0.5, 0.9, 0.95, 0.99]).unstack() #Currency'lere göre çeyreklikler

# df["Amount Received"].quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T

"""Date verisini doğru veri tipine çeviriyoruz"""

df['Timestamp'] = pd.to_datetime(df['Timestamp'])

"""Saat veya gün bazlı analizler de yapabilmek için zaman değişkenini ayırıyoruz"""

# Zaman Bağlantılı yeni featureler
df['hour'] = df['Timestamp'].dt.hour
df['day'] = df['Timestamp'].dt.day
df['weekday'] = df['Timestamp'].dt.weekday
df['month'] = df['Timestamp'].dt.month

"""Time stamp değerine normalizasyon yapılabilir mi?
df['Timestamp'] = df['Timestamp'].apply(lambda x: x.value)
df['Timestamp'] = (df['Timestamp']-df['Timestamp'].min())/(df['Timestamp'].max()-df['Timestamp'].min())
"""

# df.info()
# df.head()

# df_time_stamp=df.groupby('Timestamp').size().reset_index(name='total_transactions')

# plt.figure(figsize=(12, 6))
# sns.lineplot(data=df_time_stamp, x='Timestamp', y='total_transactions', color='green')
# plt.title('total_transactions Over Time ')
# plt.xlabel('Hour of the Day')
# plt.ylabel('total_transactions')
# plt.show()

# df_hour=df.groupby('hour').size().reset_index(name='total_transactions')

# plt.figure(figsize=(12, 6))
# sns.lineplot(data=df_hour, x='hour', y='total_transactions', color='green')
# plt.title('total_transactions Over Time (Hourly)')
# plt.xlabel('Hour of the Day')
# plt.ylabel('total_transactions')
# plt.show()

# df_day=df.groupby('day').size().reset_index(name='total_transactions')

# plt.figure(figsize=(12, 6))
# sns.lineplot(data=df_day, x='day', y='total_transactions', color='green')
# plt.title('total_transactions Over Time (Daily)')
# plt.xlabel('Day of the Month')
# plt.ylabel('total_transactions')
# plt.show()

# df_weekday=df.groupby('weekday').size().reset_index(name='total_transactions')
# df_weekday["weekday"] = df_weekday["weekday"] + 1



# plt.figure(figsize=(12, 6))
# sns.lineplot(data=df_weekday, x='weekday', y='total_transactions', color='green')
# plt.title('total_transactions Over Time (Day Base)')
# plt.xlabel('Weekday ')
# plt.ylabel('total_transactions')
# plt.show()

# plt.figure(figsize=(12, 6))
# sns.lineplot(data=df, x='hour', y='Amount Received', estimator='sum', ci=None, color='purple')
# plt.title('Amount Received Over Time (Hourly)')
# plt.xlabel('Hour of the Day')
# plt.ylabel('Amount Received sum')
# plt.show()

# plt.figure(figsize=(12, 6))
# sns.lineplot(data=df, x='hour', y='Amount Received', estimator='mean', ci=None, color='purple')
# plt.title('Amount Received Over Time (Hourly)')
# plt.xlabel('Hour of the Day')
# plt.ylabel('Amount Received mean')
# plt.show()

# plt.figure(figsize=(12, 6))
# sns.lineplot(data=df, x='hour', y='Amount Paid', estimator='sum', ci=None, color='brown')
# plt.title('Amount Paid Over Time (Hourly)')
# plt.xlabel('Hour of the Day')
# plt.ylabel('Amount Paid sum')
# plt.show()

# plt.figure(figsize=(12, 6))
# sns.lineplot(data=df, x='hour', y='Amount Paid', estimator='mean', ci=None, color='brown')
# plt.title('Amount Paid Over Time (Hourly)')
# plt.xlabel('Hour of the Day')
# plt.ylabel('Amount Paid mean')
# plt.show()

"""Not From bank ve To bank değişkenleri kategorik olmalı sayısal değer olmamalı"""

# f, ax = plt.subplots(figsize=(10,4))

# sns.countplot(x='Payment Format', data=df, ax=ax)
# ax.set_title('Distribution of Payment Format')
# ax.set_xlabel('Payment Format', fontsize=12)
# ax.set_ylabel('Count', fontsize=12)
# ax.tick_params(axis='x', labelrotation=45)
# plt.show()

# df.head()

"""KATEGORİK-SAYISAL VE HEDEF DEĞİŞKEN ANALİZİ"""

cat_cols=['Receiving Currency', 'Payment Currency', 'Payment Format', 'Is Laundering','hour','day','weekday','month']
num_cols=['Amount Received', 'Amount Paid']
cat_but_car=['Timestamp', 'Account', 'Account.1','From Bank', 'To Bank']

# # KATEGORİK DEĞİŞKENLERİN ANALİZİ
# def cat_summary(dataframe, col_name, plot=False):
#     print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),
#                         "Ratio": 100 * dataframe[col_name].value_counts() / len(dataframe)}))

#     if plot:
#         sns.countplot(x=dataframe[col_name], data=dataframe)
#         plt.show()


# for col in cat_cols:
#     cat_summary(df, col)

# df.groupby("Payment Format").agg({"Is Laundering":"sum"})

# df.groupby("Receiving Currency").agg({"Is Laundering":"sum"}) # ratio bakılacak her bir para birimi için

# # NUMERİK DEĞİŞKENLERİN ANALİZİ
# def num_summary(dataframe, numerical_col, plot=False):
#     quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]
#     print(dataframe[numerical_col].describe(quantiles).T)

#     if plot:
#         dataframe[numerical_col].hist(bins=50)
#         plt.xlabel(numerical_col)
#         plt.title(numerical_col)
#         plt.show()

#     print("#####################################")


# for col in num_cols:
#     num_summary(df, col, True)

# # KATEGORİK DEĞİŞKENLERİN TARGET'A GÖRE ANALİZİ
# def target_summary_with_cat(dataframe, target, categorical_col):
#     print(pd.DataFrame({"TARGET_MEAN": dataframe.groupby(categorical_col)[target].mean(),"TARGET_SUM": dataframe.groupby(categorical_col)[target].sum()}), end="\n\n\n")


# for col in cat_cols:
#     target_summary_with_cat(df,"Is Laundering",col)

# # NUMERİK DEĞİŞKENLERİN TARGET GÖRE ANALİZİ
# def target_summary_with_num(dataframe, target, numerical_col):
#     print(dataframe.groupby(target).agg({numerical_col: "mean"}), end="\n\n\n")

# for col in num_cols:
#     target_summary_with_num(df, "Is Laundering", col)

# df.isnull().sum()

"""AYKIRI DEĞER ANALİZİ"""

def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):
    quartile1 = dataframe[col_name].quantile(q1)
    quartile3 = dataframe[col_name].quantile(q3)
    interquantile_range = quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def check_outlier(dataframe, col_name):
    low_limit, up_limit = outlier_thresholds(dataframe, col_name)
    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):
        return True
    else:
        return False

for col in num_cols:
    print(check_outlier(df, col))

#def replace_with_thresholds(dataframe, variable, q1=0.05, q3=0.95):

    #low_limit, up_limit = outlier_thresholds(dataframe, variable, q1=0.05, q3=0.95)
    #dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    #dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

#for col in num_cols:
    #print(col, check_outlier(df, col))  # Aykırı değer var mı kontrol et
    #if check_outlier(df, col):
        #replace_with_thresholds(df, col)  # Varsa düzelt

#for col in num_cols:
    #num_summary(df, col, True)

low_limit, up_limit=outlier_thresholds(df,"Amount Received")

for col in ['Amount Received', 'Amount Paid']:
    low_limit, up_limit=outlier_thresholds(df, col)
    print(df[(df[col] > up_limit) | (df[col] < low_limit)].count())
    print(df[(df[col] > up_limit) & (df['Is Laundering'] == 1) ].count())#aykırı değer baskılama yapılmalı mı?

"""-------------------------------------------------------------------------

Numerik ve kategorik değişkenleri ayırın

İşlem türü, kanal tipi (ATM, mobil, online) gibi kategorik değişkenleri belirleyin

İşlem tutarı, bakiye gibi sayısal değişkenleri sınıflandırın

Giden ve Gelen banka hesapları hep aynı işlemler mi? Yani Giden ve Gelen Currecyler hep aynı mı? Miktarlar hep aynı mı? Kontrol et.

Kaç tip currecy var?

Giden ve Gelen Banka Hesapları işlemlerini birleştir ve 2 ayrı değişken yap,,,,,

Alıcı hesapların bilgilerini, alınan tutarı ve para birimini içeren receiving_df dosyasını oluşturun.

Ödeyen hesapların bilgilerini, ödenen tutarı ve para birimini içeren paying_df dosyasını oluşturun.

Tüm işlemlerde kullanılan para birimlerinin bir listesini oluşturun.

sklearn LabelEncoder ile ‘Ödeme Formatı’, ‘Ödeme Para Birimi’, 'Alıcı Para Birimi'ni sınıflara göre etiketleyin.

Timestamp gün ay yıla böl. En çok aklama yapılan saat veya zamanları bul.

### Yeni Değişken Üretimi

Ham veriler modellemeye uygun değil.

Aşağıdaki gibi yeni değişkenler üretmeniz beklenir:

Müşteri başına günlük işlem ortalaması

Farklı ülkelere yapılan transfer sayısı

Aynı alıcıya yapılan tekrar eden işlemler

Dönemsel işlem aktivitesi (mevsim, saatlik yoğunluk)

# **2-Data Preprocessing & Feature Engineering**

Anormal işlem miktarı adlı sütun. Kişi bazında ortalama işlem miktarı bulunur. her bir işlemi ilgili kişinin ortalama işleminden çıkarılır(veya bölünerek % hesaplanabilir) ve aradaki fark bulunur. Bir hesabın daha önceki işlemlerine göre mevcut işlem miktarının sapması. (Örn: mevcut_tutar - ortalama_tutar)

İşlem sıklığı hesaplanabilir.

Timestamp gün ay yıla bölerek yeni değişkenler oluşturabiliriz. qcut ile de yapılabilir

Extracting relevant time-based features



*   df['hour'] = df['Timestamp'].dt.hour
*   df['day'] = df['Timestamp'].dt.day
*   df['weekday'] = df['Timestamp'].dt.weekday
*   df['month'] = df['Timestamp'].dt.month

Gece Yarısı işlemi(binary) gece yapılan işlemlere 1 olmayanlara 0 diyebiliriz

Bu aşamada tekrardan standartlaştırma işlemi yeni değişkenlere yapılabilir ve numeric,kategorik ve kardinal değişken listesi güncellenebilir

Amount Received ve Amount Paid kolonları arasındaki fark. Normal bir işlemde bu değerler aynı olmalıdır.

Receiving Currency ve Payment Currency kolonlarının aynı olup olmadığına dair ikili (binary) bir değişken. Farklı para birimi kullanımı bir sinyal olabilir.

Payment Format (örneğin 'Reinvestment', 'Cheque', 'Credit Card') için ikili değişkenler oluşturun (One-Hot Encoding).
"""

df.head()

df['unique_senders'] = df.groupby('Account')['Account.1'].transform('nunique')# 1 hesap kaç farklı hesaba işlem gerçekleştirmiş
df['unique_receivers'] = df.groupby('Account.1')['Account'].transform('nunique')#1 hesap kaç farklı hesaptan işlem almış

df['self_loops'] = (df['Account'] == df['Account.1']).astype(int)#kendi hesabına işlem yapmış mı?
df['self_loop_count'] = df.groupby('Account')['self_loops'].transform('sum')#kendine yaptığı işlem sayısı

df.head()

# Step 1: Define Transaction Count (Total number of transactions per Account)
df['transaction_count'] = df.groupby('Account')['Amount Paid'].transform('count')# her bir hesabın  toplam ödediği miktar

# Step 2: Standard deviation of Amount Paid and Amount Received per Account
df['std_amount_sent'] = df.groupby('Account')['Amount Paid'].transform('std').fillna(0)#her bir hesabın  ödediği miktarın standart sapması
df['std_amount_received'] = df.groupby('Account')['Amount Received'].transform('std').fillna(0)#her bir hesaba gelen miktarın standart sapması

# Step 3: Maximum and Mean of Amount Paid per Account
df['max_amount_sent'] = df.groupby('Account')['Amount Paid'].transform('max')#her bir hesabın ödediği max tutar
df['mean_amount_sent'] = df.groupby('Account')['Amount Paid'].transform('mean').replace(0, 1)  #her bir hesabın ödediği ortalama tutar

# Step 4: Calculate the Largest Transaction Ratio (max_amount_sent / mean_amount_sent)
df['largest_transaction_ratio'] = df['max_amount_sent'] / df['mean_amount_sent']#her bir hesabın gönderdiği max işlemin ortalamasına oranı

# Step 5: Large Transaction Count (Transactions where Amount Paid is greater than the 90th percentile)
threshold = df['Amount Paid'].quantile(0.90)
df['large_transaction_count'] = df.groupby('Account')['Amount Paid'].transform(lambda x: (x > threshold).sum())

# Step 6: Calculate Large Transaction Ratio (large_transaction_count / transaction_count)
df['large_transaction_ratio'] = df['large_transaction_count'] / df['transaction_count']

# Step 7: Count of Same Amount Transactions (same Amount Paid by the same Account)???
df['same_amount_transactions'] = df.groupby(['Account','Amount Paid'])['Amount Paid'].transform('count')#bir hesap haç farklı miktarda ödeme yaptı

df.head()

df[df["Is Laundering"]==1].groupby(["same_amount_transactions"]).agg({"Amount Paid":["sum","max"]})

df[df["Is Laundering"]==1]["same_amount_transactions"].value_counts()



df = df.sort_values(by=['Account', 'Timestamp'])
df['date'] = df['Timestamp'].dt.date
daily_transactions = df.groupby(['Account', 'date']).size().reset_index(name='transactions_per_day')

df['time_gap'] = df.groupby('Account')['Timestamp'].diff().dt.total_seconds().fillna(0)#her bir hesabın bir önceki işlemi ile arasında geçen süre
df['min_time_gap'] = df.groupby('Account')['time_gap'].transform('min')#her bir hesabın bir önceki işlemi ile arasında geçen min süre
df['max_time_gap'] = df.groupby('Account')['time_gap'].transform('max').replace(0, 1)#her bir hesabın bir önceki işlemi ile arasında geçen max süre
df['gap_ratio'] = df['min_time_gap'] / df['max_time_gap']

df['hour'] = df['Timestamp'].dt.hour
df['time_of_day'] = pd.cut(df['hour'], bins=[0,6,12,18,24], labels=['Night', 'Morning', 'Afternoon', 'Evening'],right=False)

# 1. Handle mode calculation safely for 'most_active_time'
df['most_active_time'] = df.groupby('Account')['time_of_day'].transform(
    lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'  # Handle empty mode
)#her bir hesabın en çok hangi zamanda işlem yaptığı bilgisi

df['is_weekend'] = df['Timestamp'].dt.weekday >= 5
df['weekend_ratio'] = df.groupby('Account')['is_weekend'].transform('mean')#her hesabın işlemlerini ortalama ne kadarını hafta sonu gerçekleştiriyor

# 'is_night' sütununu 'isin' ile oluşturma
night_hours = [20, 21, 22, 23, 24, 0, 1, 2, 3, 4, 5, 6]
df['is_night'] = df['hour'].isin(night_hours)
df['night_ratio'] = df.groupby('Account')['is_night'].transform('mean')#her hesabın işlemlerini ortalama ne kadarını gece gerçekleştiriyor
# Gece saatlerini liste olarak tanımlama


df['is_burst'] = df['time_gap'] < 600
df['burst_transaction_count'] = df.groupby('Account')['is_burst'].transform('sum')

# Step 1: Detect Cross-Border Transactions
# Cross-border transactions occur when the 'Receiving Currency' differs from the 'Payment Currency'
df['cross_border'] = (df['Receiving Currency'] != df['Payment Currency']).astype(int)#eğer alınan miktarın par birimi ile ödenen miktarın para birimi eşit değilse yurt dışına işlem yapmış sayıyoruz.YUrt dışı işlemi ise 1 değilse 0

# Step 2: Calculate the ratio of cross-border transactions for each account
df['cross_border_ratio'] = df.groupby('Account')['cross_border'].transform('mean')#her bir hesabın yaptığı işlemler içinde "yurtdışı işlem" oranını

# Step 3: Detect Same-Bank Transactions
# Same-bank transactions occur when the 'From Bank' matches the 'To Bank'
df['same_bank_transfer'] = (df['From Bank'] == df['To Bank']).astype(int)# eğer aynı bankaya transfer ettiyse 1 değilse 0

# Step 4: Calculate the ratio of same-bank transactions for each account
df['same_bank_ratio'] = df.groupby('Account')['same_bank_transfer'].transform('mean')#her bir hesabın yaptığı işlemler içinde aynı bankaya yaptığı işlem oranı

df.describe().T

df.head()

"""İşlem başına fraud oranı. 1'den fazla fraud yapmış kişileri bulur."""

# 2. Günlük şüpheli işlem sayısını hesapla
fraud_per_day = df[df["Is Laundering"] == 1].groupby("date").size().reset_index(name="fraud_count")

# 3. Tüm işlem sayısıyla birleştirmek istersen:
total_per_day = df.groupby("date").size().reset_index(name="total_count")

# 4. Merge ederek fraud oranı oluştur
daily_stats = pd.merge(total_per_day, fraud_per_day, on="date", how="left")
daily_stats["fraud_count"] = daily_stats["fraud_count"].fillna(0)
daily_stats["fraud_ratio"] = daily_stats["fraud_count"] / daily_stats["total_count"]


# 5. Gerekirse ana df'e de fraud oranı olarak join edebilirsin:
df = df.merge(daily_stats[["date", "fraud_ratio"]], on="date", how="left")

df.groupby("fraud_ratio").agg({"Is Laundering":"sum"})

cat_cols=['Receiving Currency', 'Payment Currency', 'Payment Format', 'Is Laundering','hour','day','weekday','month',"self_loops","time_of_day","most_active_time","is_weekend","is_night","is_burst","cross_border","same_bank_transfer","date"]
num_cols=['Amount Received', 'Amount Paid',"unique_senders","unique_receivers","self_loop_count","transaction_count","std_amount_sent","std_amount_received","max_amount_sent","mean_amount_sent","largest_transaction_ratio","large_transaction_count","large_transaction_ratio","same_amount_transactions","time_gap","min_time_gap","max_time_gap","gap_ratio","weekend_ratio","night_ratio","burst_transaction_count","cross_border_ratio","same_bank_ratio","fraud_ratio",'Payment_Currency_risk', 'Receiving_Currency_risk']
cat_but_car=['Timestamp', 'Account', 'Account.1','From Bank', 'To Bank']

df.head()

#elenecek featurelar

#Encode categorical variables(label encoding)

#Standardize numeric columns(Robustscaler())

"""## Timestamp robust scaler veya min max normalization
## 06.08.2025

Ağaç yöntem kullanılacağı için `RobustScaler`'a gerek yok.


---



Yüksek kardinal değerli kategorik değişkenlerimiz var *(Banka ID'leri, tarih formatları gibi) (High Cardinality Categorical Variables)*

Bu değişkenler için `LabelEncoder` kullanılabilir (Test edilerek)

**Önemli**: `LabelEncoder` sıralı bir anlam taşıyacağından ve yeni feature edinimleri

yetersiz kalırsa test edilerek kullanılmalı


**YANİ LABELENCODING OPSIYONELDIR**


---



Kardinalitesi düşük kategorik değişkenler için ise

`OneHotEncoder` kullanılmalı

Çarpık değerli sütunlarımız var ancak ağaç yöntemleri kullanacağımızdan Loglamaya gerek yok.


---



Ağaç yöntemleri kullanılacağından **Dengesiz Veri Seti Çözümü** için:

SMOTE ve Undersampling-Oversampling yerine:
**XGBoost** için `scale_pos_weight = 980`

**RandomForest** için `class_weight='balanced`
Kullanılmalı

---


## `ONE-HOT-ENCODING` SÜTUNLARININ SEÇİLMESİ

## RAM TÜKETİMİNİ MİN. İNDİRGEMEK İÇİN SÜTUN DÜZENLEMESİ
"""

cat_cols #is laundering cat colsa dahil olmuş
cat_cols = [col for col in cat_cols if col not in 'Is Laundering'] #böylece çıkarttık

ohe_cols = cat_cols #onehotencoding sütunlarımızı seçelim.

[col for col in df.columns if df[col].dtype not in [int,float] and df[col].nunique() == 2] #bool tipindeler bunlar ohe_cols'a eklenmeyecek.

[col for col in df.columns if df[col].dtype in [int,float] and df[col].nunique() == 2]

[col for col in df.columns if df[col].nunique() == 2]

cat_cols.remove('day')

for i in cat_cols:
  print(i," :", df[i].dtype)
#Bu çıkan sonucu chatgpt'ye sordum ve bool olanlara OHE yapılmaması gerektiğini söyledi.

for i in ohe_cols:
  print(i," :", df[i].nunique()) #binary var mı diye kontrol ediyoruz
  #binaryleri çıkaralım aşağıda
  #time_of_day akşam-sabah-öğle diye gidiyor o yüzden hour'a gerek yok.

binary_cols = ['Is Laundering', 'self_loops', 'cross_border', 'same_bank_transfer', 'is_weekend', 'is_burst','is_night','month']

ohe_cols = [col for col in cat_cols if col not in binary_cols]

ohe_cols = [col for col in ohe_cols if col != ["is_night","is_weekend","is_night","is_burst","cross_border","same_bank_transfer","date"]]

df["date"] = pd.to_datetime(df['date'])

cat_but_car

ohe_cols.append('date_half')

#ohe_cols.remove('day')

#ohe_cols.remove('date')

df.groupby('Receiving Currency')['Is Laundering'].agg(['mean', 'sum', 'count']).sort_values(by='mean', ascending=False)

agg = df.groupby('Receiving Currency')['Is Laundering'].agg(['mean', 'sum', 'count'])

# 1. Risk seviyelerini qcut ile sınıflandır
agg['risk_score'] = agg['mean'] * np.log(agg['count'])
agg['risk_level'] = pd.qcut(agg['risk_score'], q=3, labels=[1, 2, 3])  # 1: low, 2: medium, 3: high

agg.head()

currency_risk_map = agg['risk_level'].astype(int).to_dict()
df['Receiving_Currency_risk'] = df['Receiving Currency'].map(currency_risk_map)

# 1. Payment Currency bazında laundering oranı ve sayılar
agg_paid = df.groupby('Payment Currency')['Is Laundering'].agg(['mean', 'sum', 'count'])

# 2. Risk skorunu hesapla
agg_paid['risk_score'] = agg_paid['mean'] * np.log(agg_paid['count'])

# 3. 3 gruba ayır: 1 = low, 2 = medium, 3 = high
agg_paid['risk_level'] = pd.qcut(agg_paid['risk_score'], q=3, labels=[1, 2, 3])

# 4. Sayısal hale getir ve veri setine ekle
payment_currency_risk_map = agg_paid['risk_level'].astype(int).to_dict()
df['Payment_Currency_risk'] = df['Payment Currency'].map(payment_currency_risk_map)

print(df['Payment_Currency_risk'].dtype)
print(df['Receiving_Currency_risk'].dtype)

df.groupby('Receiving_Currency_risk')['Is Laundering'].agg(['mean', 'sum', 'count']).sort_values(by='mean', ascending=False)

ohe_cols.remove('Receiving Currency')

ohe_cols.remove('Payment Currency')

ohe_cols

df['date_half'] = df['date'].apply(lambda x: 'start' if x.day <= 15 else 'mid') #date sütununu ay başı ve ay ortası olarak ayırma

ohe_cols.remove('date') #date_half olduğu için.

ohe_cols.remove('hour') #most_active_time olduğu için silinir

for i in ohe_cols:
  print(i," :", df[i].nunique())

df.head()

# Time ve Amount değişkenlerini standartlaştırma rob_scaler = RobustScaler()
#df['Amount Received'] = rob_scaler.fit_transform(df['Amount Received'].values.reshape(-1,1))
#df['Amount Paid'] = rob_scaler.fit_transform(df['Amount Paid'].values.reshape(-1,1))
#df['Timestamp'] = rob_scaler.fit_transform(df['Timestamp'].values.reshape(-1,1))
#df.head()

def one_hot_encoder(dataframe, categorical_cols, drop_first=False):
    """
    Kategorik değişkenleri one-hot encoding ile sayısal hale getiren fonksiyon

    Parametreler:
    dataframe: Dönüştürülecek veri seti
    categorical_cols: Dönüştürülecek kategorik değişkenler
    drop_first: İlk kategoriyi düşürüp düşürmeyeceği (multicollinearity'yi önler)

    One-hot encoding:
    - Her kategorik değeri ayrı bir binary sütun haline getirir
    - Örnek: Gender -> Gender_Male, Gender_Female
    """
    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first, sparse=False, dtype='int8')
    return dataframe

df_with_ohe = one_hot_encoder(df, ohe_cols, drop_first=True)

df_with_ohe.head()

df_with_ohe.drop(cat_but_car, axis=1, inplace=True)

df_with_ohe.drop(["month","Payment Currency","Receiving Currency", "date"], axis=1, inplace=True)

#correlation_matrix_ohe = df_with_ohe[num_cols].corr()

#plt.figure(figsize=(15, 8))
#sns.heatmap(correlation_matrix_ohe, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
#plt.title('Correlation Matrix of Selected Features with Target (Is Laundering)')
#plt.tight_layout()
#plt.show()

drop_cols = ["burst_transaction_count", "large_transaction_count", "Receiving_Currency_risk", "mean_amount_sent", "transaction_count", "largest_transaction_ratio", "std_amount_received"]

for i in drop_cols:
  df_with_ohe.drop(i, axis=1, inplace=True)
  num_cols.remove(i)

csv_seyi = df_with_ohe.to_csv("df_with_ohe.csv")

df["unique_senders"].describe().T

df_with_ohe.head()

df.info

df_with_ohe.describe().T

from google.colab import drive
drive.mount('/content/drive')

check_df(df_with_ohe)

df_with_ohe.describe().T

df_with_ohe.head()

"""# **3. Base Models**

Model öncesi customerID gibi kardinal değişkenlerin çıkarılması gerekiyor

Bağımlı değişken bağımsız değişken ayırma

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2, # %20 test verisi
                                                    random_state=42, # Tekrarlanabilirlik için
                                                    stratify=y) # Sınıf dengesini korumak için
                                          
Bu kodda dikkat edilmesi gereken en önemli parametre stratify=y'dir. Bu parametre, veri bölme işlemi sırasında hedef değişkendeki sınıf dağılımının (oranının) hem eğitim hem de test setinde aynı kalmasını sağlar. Bu, özellikle sınıf dengesizliğinin olduğu veri setlerinde büyük önem taşır.

Miuul Dengesiz Veri Seti yazısından yararlanabiliriz


*   Resampling
*   Oversampling
*   Undersampling

Önemli Not: Sınıf dengesizliği ile başa çıkma yöntemleri (SMOTE, Undersampling vb.), yalnızca eğitim verisi (X_train, y_train) üzerinde uygulanmalıdır. Ağaç yönteminde sampling işlemlerine gerek yoktur

Kullanılabilecek sınıflandırma problemine uygun modeller:



1.   Karar Ağaçları (Decision Trees) ve Rastgele Ormanlar (Random Forests):
2.   Gradient Boosting Modelleri (XGBoost, LightGBM, CatBoost)
3.   Lojistik Regresyon(korelasyonu yüksek olanları çıkar)

Hangi Başarı Metriğini Seçeceğiz
*  Confusion Matrix
  * Precision
  * Recall
  * F1 Score
* ROC Curve
* AUC

Hiper parametre optimizasyonu
"""

cat_but_car

#correlation_matrix_ohe = df_with_ohe[num_cols].corr()

#correlation_matrix_ohe

#plt.figure(figsize=(25, 15))
#sns.heatmap(correlation_matrix_ohe, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
#plt.title('Correlation Matrix of Selected Features with Target (Is Laundering)')
#plt.tight_layout()
#plt.show()

#X = df.drop("Is Laundering", axis=1)
#y = df["Is Laundering"]

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

#model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
#model.fit(X_train, y_train)

#y_pred = model.predict(X_test)
#y_proba = model.predict_proba(X_test)[:, 1]

#sample_idx = np.random.choice(len(X), size=500_000, replace=False)
